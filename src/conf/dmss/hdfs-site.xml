<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!--*************************HDFS HA**************************************-->
    <property>
        <name>dfs.nameservices</name>
        <value>dmss</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.dmss</name>
        <value>nn1,nn2</value>
    </property>
    <!--指定NameNode的hostname:port-->
    <property>
        <name>dfs.namenode.rpc-address.dmss.nn1</name>
        <value>master:9000</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.dmss.nn2</name>
        <value>master2:9000</value>
    </property>
    <!--指定NameNode的HTTP服务器端口-->
    <property>
        <name>dfs.namenode.http-address.dmss.nn1</name>
        <value>master:50070</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.dmss.nn2</name>
        <value>master2:50070</value>
    </property>
    <!--the URI which identifies the group of JNs where the NameNodes will write/read edits-->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://slaver1:8485;slaver2:8485;slaver3:8485;slaver4:8485;slaver5:8485/dmss</value>
    </property>
    <!--the Java class that HDFS clients use to contact the Active NameNode（HDFS Client连接NameNode所使用的类，Client可以通过此类>来判定哪个NameNode为Alive，并与它保持通信）-->
    <property>
        <name>dfs.client.failover.proxy.provider.dmss</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <!--a list of scripts or Java classes which will be used to fence the Active NameNode during a failover（指定Failover期间杀死可能存活的NameNode的方法）-->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property>
    <!--the path where the JournalNode daemon will store its local state-->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/opt/hadoop272/hdfs/journalnode</value>
    </property>
    <!--This specifies that the cluster should be set up for automatic failover-->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

    <!--**************************************************************************-->

    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!--namenode set-->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/opt/hadoop272/hdfs/name1,/opt/hadoop272/hdfs/name2</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/opt/hadoop272/hdfs/data1,/opt/hadoop272/hdfs/data2</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
